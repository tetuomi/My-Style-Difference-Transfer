{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from os.path import basename\n",
    "from os.path import join\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5791e-09, 2.1640e-08, 2.5100e-08, 6.6962e-09, 3.2617e-07, 1.8972e-11,\n",
      "         7.3669e-08, 1.9988e-08, 4.7147e-03, 9.0731e-08, 3.3016e-08, 9.9518e-01,\n",
      "         8.4838e-13, 1.5790e-09, 5.2730e-12, 5.4941e-11, 6.6949e-08, 1.9095e-09,\n",
      "         2.8374e-08, 1.0302e-09, 2.6120e-09, 2.1241e-09, 6.2753e-14, 3.7070e-09,\n",
      "         5.2187e-08, 1.0815e-04]], device='cuda:0')\n",
      "class L\n",
      "tensor(20.2664, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from utility.utility import load_images\n",
    "from utility.vgg_network_with_top import VGG as VGGWithTOP\n",
    "\n",
    "f = 'output/265/[1020].jpg'\n",
    "img = load_images(f, 256, 'cuda:0', 1)\n",
    "# print(img)\n",
    "# vgg_with_top = VGGWithTOP()\n",
    "# vgg_with_top.load_state_dict(torch.load('./vgg_with_top.pth'))\n",
    "from torchvision import models\n",
    "vgg_with_top = models.resnet18(weights=None)\n",
    "vgg_with_top.fc = nn.Linear(512, 26)\n",
    "vgg_with_top.load_state_dict(torch.load('./resnet_0_1.pth'))\n",
    "\n",
    "for param in vgg_with_top.parameters():\n",
    "    param.requires_grad = False\n",
    "vgg_with_top.to('cuda:0')\n",
    "vgg_with_top.eval()\n",
    "\n",
    "content_class = 'A'\n",
    "label = torch.tensor([i for i in range(26) if chr(i+ord('A')) == content_class]).to('cuda:0')\n",
    "# logit = vgg_with_top(img, ['fc3'])[0]\n",
    "logit = vgg_with_top(img)\n",
    "# print(logit)\n",
    "prob = nn.Softmax(dim=1)(logit)\n",
    "print(prob)\n",
    "pred_label = torch.max(prob, 1)[1].item()\n",
    "print('class', chr(ord('A') + pred_label))\n",
    "loss = nn.CrossEntropyLoss()(logit, label)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SIZE:  41990\n",
      "VAL SIZE:  13624\n",
      "TEST SIZE:  25714\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1608, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5412, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0157, 0.9059, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.3020, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.6863, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0824, 0.9804, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.4431, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.8196, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1961,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5725,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 0.9725],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0196, 0.9216,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 0.6667],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3216, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 0.2980],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6980, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         0.9137, 0.0157],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0863, 0.9843, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         0.5608, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4471, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         0.1882, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8196, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8196,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1961, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.4510,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5647, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9882, 0.0941,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.9098, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7137, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2863, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.3412, 0.0000,\n",
      "         0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "from train_vgg import make_dataloader\n",
    "\n",
    "d = make_dataloader()\n",
    "print(d['test'].dataset[0][0][0][100:120,100:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SIZE:  41990\n",
      "VAL SIZE:  13624\n",
      "TEST SIZE:  25714\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000001?line=4'>5</a>\u001b[0m d \u001b[39m=\u001b[39m make_dataloader(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000001?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize \u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000001?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, l \u001b[39min\u001b[39;00m d[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000001?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _i, _l \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(i, l): \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000001?line=8'>9</a>\u001b[0m         plt\u001b[39m.\u001b[39msubplot(\u001b[39m4\u001b[39m, \u001b[39m7\u001b[39m, _l\u001b[39m.\u001b[39mitem()\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torch/utils/data/dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    691\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    694\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/works/research/Style-Difference-Transfer/train_vgg.py:58\u001b[0m, in \u001b[0;36mLoadDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     56\u001b[0m filepath \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_list[index]\n\u001b[1;32m     57\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(filepath)\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m img_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m     59\u001b[0m alphabet \u001b[39m=\u001b[39m path\u001b[39m.\u001b[39mbasename(filepath)[:\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m]\n\u001b[1;32m     60\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mord\u001b[39m(alphabet) \u001b[39m-\u001b[39m \u001b[39mord\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from train_vgg import make_path_list, make_dataloader\n",
    "\n",
    "# t, _, _ = make_path_list()\n",
    "# print(t[:10])\n",
    "d = make_dataloader('cpu')\n",
    "plt.figure(figsize =(10, 10))\n",
    "for i, l in d['train']:\n",
    "    for _i, _l in zip(i, l): \n",
    "        plt.subplot(4, 7, _l.item()+1)\n",
    "        plt.imshow(_i[0], cmap='gray')\n",
    "\n",
    "plt.figure(figsize =(10, 10))\n",
    "for i, l in d['val']:\n",
    "    for _i, _l in zip(i, l): \n",
    "        plt.subplot(4, 7, _l.item()+1)\n",
    "        plt.imshow(_i[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SIZE:  41990\n",
      "VAL SIZE:  13624\n",
      "TEST SIZE:  25714\n",
      "i tensor([[[[-0.5423,  0.3065, -0.3376,  ..., -0.4974,  0.8768, -0.1035],\n",
      "          [ 0.5822, -1.0294,  0.9340,  ..., -0.1026, -0.9206, -0.1713],\n",
      "          [ 0.6879, -2.0340,  0.6456,  ..., -0.3766, -0.7715,  0.8524],\n",
      "          ...,\n",
      "          [ 1.7260,  1.2476, -0.6634,  ..., -2.3865,  0.3136,  0.0300],\n",
      "          [-0.4887, -0.4332,  0.0106,  ...,  0.6427, -0.3805, -0.1196],\n",
      "          [-0.0113,  0.9679, -2.3826,  ..., -1.8271,  0.2571,  0.6796]]],\n",
      "\n",
      "\n",
      "        [[[-0.5027, -1.1181,  1.0078,  ..., -0.8235,  0.5323, -1.1458],\n",
      "          [-1.0572,  1.1472,  0.2468,  ...,  1.1336,  0.1664, -0.9696],\n",
      "          [-0.9418, -0.1571, -0.3978,  ...,  1.3375,  0.2424,  0.0682],\n",
      "          ...,\n",
      "          [-0.9631, -1.0917,  1.7891,  ..., -1.3390,  0.6551,  1.1878],\n",
      "          [-0.2230,  0.2640, -0.2684,  ..., -0.8585, -0.1678, -1.2309],\n",
      "          [-1.1352,  0.5258,  0.1004,  ...,  2.0580,  0.0970, -0.8208]]],\n",
      "\n",
      "\n",
      "        [[[-1.6625,  0.9490,  1.8656,  ..., -1.8982,  1.7411,  0.3963],\n",
      "          [-0.7133, -1.4019,  1.1369,  ..., -0.9975,  1.1331, -0.7550],\n",
      "          [-0.0642,  0.3218, -0.0917,  ...,  0.5686, -0.5694,  1.1757],\n",
      "          ...,\n",
      "          [-0.1513, -0.2195, -0.1939,  ...,  1.9498, -0.9619,  0.6784],\n",
      "          [ 0.6543,  0.6269,  0.0578,  ...,  0.2402,  0.3461,  0.3772],\n",
      "          [-1.1519, -1.7594,  0.2500,  ...,  1.5730,  0.2474, -0.7736]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.8287, -0.4754, -0.1954,  ...,  1.0040,  0.5933, -0.5574],\n",
      "          [ 1.4506, -0.5111, -0.1198,  ..., -0.2150,  0.1655,  0.2571],\n",
      "          [-0.6803, -0.3366,  0.1426,  ..., -0.6580,  0.3054,  0.8554],\n",
      "          ...,\n",
      "          [-0.3221, -0.8873,  0.0183,  ...,  0.5928,  1.7622,  0.7421],\n",
      "          [-0.4659,  0.4088, -0.9953,  ..., -0.3493, -2.6852,  0.3789],\n",
      "          [ 0.2998, -1.0955,  0.8505,  ...,  0.0906,  1.2650,  1.5996]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3821, -0.1671,  0.5031,  ...,  0.2560, -0.3141, -0.2441],\n",
      "          [-0.3634, -0.3271, -1.5936,  ...,  0.0592, -1.2590,  0.2881],\n",
      "          [ 0.5435,  0.6737, -1.0612,  ...,  2.7391,  0.5299, -2.0136],\n",
      "          ...,\n",
      "          [ 1.5568,  0.3187, -0.6020,  ...,  1.1930,  0.5432,  0.4547],\n",
      "          [ 0.6651,  0.8410, -0.0999,  ...,  0.9296,  0.1281,  1.6398],\n",
      "          [-0.4841, -0.1612,  0.2045,  ...,  0.1016, -1.2534,  1.0817]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5120,  1.0903, -0.2918,  ...,  1.2709, -0.0378,  0.3350],\n",
      "          [-0.0050, -0.3147,  1.5113,  ..., -2.2863, -0.6311, -0.7261],\n",
      "          [ 1.7034,  0.0785, -0.1993,  ...,  0.5745, -1.1855, -0.6811],\n",
      "          ...,\n",
      "          [ 0.0909,  0.1733,  0.0605,  ...,  1.1779,  1.5189,  0.4303],\n",
      "          [-0.5361, -0.5759, -0.1050,  ..., -1.9115, -1.5877, -0.1922],\n",
      "          [-0.0835,  0.6974,  1.5129,  ...,  0.2917,  0.3700, -0.2230]]]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 3, 3], expected input[10, 1, 256, 256] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000001?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m'\u001b[39m, i)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000001?line=19'>20</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000001?line=20'>21</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mout1\u001b[39m\u001b[39m'\u001b[39m, vgg(i, [\u001b[39m'\u001b[39;49m\u001b[39mfc3\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000001?line=21'>22</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mout2\u001b[39m\u001b[39m'\u001b[39m, vgg2(i))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000001?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, l \u001b[39min\u001b[39;00m d[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/works/research/Style-Difference-Transfer/utility/vgg_network_with_top.py:63\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, x, out_keys)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, out_keys):\n\u001b[1;32m     62\u001b[0m     out \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> 63\u001b[0m     out[\u001b[39m'\u001b[39m\u001b[39mr11\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1_1(x))\n\u001b[1;32m     64\u001b[0m     out[\u001b[39m'\u001b[39m\u001b[39mr12\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1_2(out[\u001b[39m'\u001b[39m\u001b[39mr11\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m     65\u001b[0m     out[\u001b[39m'\u001b[39m\u001b[39mp1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool1(out[\u001b[39m'\u001b[39m\u001b[39mr12\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 3, 3], expected input[10, 1, 256, 256] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "from utility.vgg_network_with_top import VGG, Model\n",
    "from train_vgg import make_path_list, make_dataloader\n",
    "\n",
    "d = make_dataloader('cpu')\n",
    "vgg = VGG(26)\n",
    "vgg.eval()\n",
    "\n",
    "# vgg2 = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', init_weights=False)\n",
    "# vgg2.features[0] = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "# vgg2.avgpool = nn.AdaptiveAvgPool2d((8, 8))\n",
    "# vgg2.classifier[0] = nn.Linear(512*8*8, 4096)\n",
    "# vgg2.classifier[6] = nn.Linear(4096, 26)\n",
    "# vgg2.eval()\n",
    "\n",
    "# m = Model()\n",
    "# m.eval()\n",
    "\n",
    "i = torch.randn(10, 1, 256, 256)\n",
    "print('i', i)\n",
    "with torch.no_grad():\n",
    "    print('out1', vgg(i, ['fc3']))\n",
    "    print('out2', vgg2(i))\n",
    "\n",
    "for i, l in d['train']:\n",
    "    o1 = vgg(i, ['fc3'])\n",
    "    # o2 = vgg2(i)\n",
    "    break\n",
    "\n",
    "print(o1)\n",
    "# print(o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.unsqueeze(torch.tensor(0), dim=0)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-123.6800)\n",
      "tensor(151.0610)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "img should be Tensor Image. Got <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000002?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(i\u001b[39m.\u001b[39mmax())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000002?line=26'>27</a>\u001b[0m i \u001b[39m=\u001b[39m max_min_scale(i)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000002?line=27'>28</a>\u001b[0m prep(i)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000002?line=28'>29</a>\u001b[0m \u001b[39mprint\u001b[39m(i\u001b[39m.\u001b[39mmin())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000002?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(i\u001b[39m.\u001b[39mmax())\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torchvision/transforms/transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 94\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torchvision/transforms/transforms.py:269\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    262\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torchvision/transforms/functional.py:358\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    356\u001b[0m     _log_api_usage_once(normalize)\n\u001b[1;32m    357\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 358\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be Tensor Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    360\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mnormalize(tensor, mean\u001b[39m=\u001b[39mmean, std\u001b[39m=\u001b[39mstd, inplace\u001b[39m=\u001b[39minplace)\n",
      "\u001b[0;31mTypeError\u001b[0m: img should be Tensor Image. Got <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from utility.utility import load_images, max_min_scale\n",
    "\n",
    "f = '../font2img/image/Abel-Regular/D.png'\n",
    "# f = '/home/tetta/Pictures/tetta_kondo.JPG'\n",
    "i = load_images(f, 256, 'cpu', 1)\n",
    "\n",
    "# print(i.shape)\n",
    "# print(i[0][0][0:5, 0:5])\n",
    "# print(i[0][0][100:120, 80:90])\n",
    "# plt.imshow(i[0].permute(1,2,0))\n",
    "# print(i.dtype)\n",
    "\n",
    "prep = transforms.Compose([\n",
    "                        # transforms.Resize((256,256)),\n",
    "                        # transforms.RandomRotation(angle),\n",
    "                        # transforms.ToTensor(),\n",
    "                        # transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to BGR\n",
    "                        transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], #subtract imagenet mean\n",
    "                                                std=[1,1,1]),\n",
    "                        transforms.Lambda(lambda x: x.mul_(255)),\n",
    "                        ])\n",
    "\n",
    "print(i.min())\n",
    "print(i.max())\n",
    "\n",
    "# i = max_min_scale(i)\n",
    "prep(i)\n",
    "print(i.min())\n",
    "print(i.max())\n",
    "# plt.imshow(i[0].permute(1,2,0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits tensor([[-0.1058,  0.1741, -0.0923, -0.0151, -0.2229, -0.2300, -0.0282, -0.0360,\n",
      "          0.0891,  0.0046,  0.2404, -0.0240, -0.0099,  0.0192,  0.0455,  0.1102,\n",
      "         -0.0802,  0.0678, -0.0605, -0.0148,  0.0418, -0.0833, -0.0018,  0.1328,\n",
      "          0.0070, -0.1701],\n",
      "        [-0.1210,  0.0962,  0.0478, -0.1091, -0.1825, -0.2050, -0.0410, -0.0358,\n",
      "         -0.0169, -0.1034,  0.1817,  0.0946, -0.0660, -0.1040, -0.1219, -0.0929,\n",
      "         -0.0100,  0.1631, -0.1130, -0.0129,  0.1019, -0.0275, -0.1003, -0.0932,\n",
      "          0.0661, -0.1777]], grad_fn=<AddmmBackward0>)\n",
      "pred tensor([[0.0347, 0.0459, 0.0352, 0.0380, 0.0309, 0.0307, 0.0375, 0.0372, 0.0422,\n",
      "         0.0388, 0.0491, 0.0377, 0.0382, 0.0393, 0.0404, 0.0431, 0.0356, 0.0413,\n",
      "         0.0363, 0.0380, 0.0402, 0.0355, 0.0385, 0.0441, 0.0389, 0.0326],\n",
      "        [0.0352, 0.0437, 0.0417, 0.0356, 0.0331, 0.0324, 0.0381, 0.0383, 0.0391,\n",
      "         0.0358, 0.0476, 0.0437, 0.0372, 0.0358, 0.0352, 0.0362, 0.0393, 0.0468,\n",
      "         0.0355, 0.0392, 0.0440, 0.0387, 0.0359, 0.0362, 0.0424, 0.0333]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "logits tensor([[-0.0348,  0.0225, -0.0269, -0.0461,  0.0233,  0.0748, -0.0116, -0.0346,\n",
      "          0.1197,  0.0875, -0.0025, -0.1038,  0.0362, -0.1062,  0.0056,  0.1152,\n",
      "         -0.1096,  0.0059, -0.0523,  0.0188,  0.0035, -0.0186, -0.0559, -0.0624,\n",
      "         -0.0650, -0.1030],\n",
      "        [ 0.0552, -0.0111, -0.0155, -0.1681, -0.1512,  0.1471, -0.0802,  0.0462,\n",
      "          0.2316,  0.1131, -0.0462, -0.0679, -0.0467, -0.0067,  0.0757,  0.1023,\n",
      "          0.0061, -0.1386, -0.0464, -0.0758, -0.0348,  0.0315, -0.1796,  0.0482,\n",
      "          0.0512, -0.1800]], grad_fn=<AddmmBackward0>)\n",
      "pred tensor([[0.0375, 0.0397, 0.0378, 0.0371, 0.0398, 0.0419, 0.0384, 0.0375, 0.0438,\n",
      "         0.0424, 0.0388, 0.0350, 0.0403, 0.0349, 0.0391, 0.0436, 0.0348, 0.0391,\n",
      "         0.0369, 0.0396, 0.0390, 0.0381, 0.0367, 0.0365, 0.0364, 0.0351],\n",
      "        [0.0410, 0.0383, 0.0382, 0.0328, 0.0333, 0.0449, 0.0358, 0.0406, 0.0489,\n",
      "         0.0434, 0.0370, 0.0362, 0.0370, 0.0385, 0.0418, 0.0429, 0.0390, 0.0337,\n",
      "         0.0370, 0.0359, 0.0374, 0.0400, 0.0324, 0.0407, 0.0408, 0.0324]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from utility.vgg_network_with_top import VGG\n",
    "import torch\n",
    "\n",
    "vgg = VGG(26)\n",
    "# i = torch.ones(12, 1, 256, 256)\n",
    "i = torch.randn(2, 1, 256, 256)\n",
    "o = vgg(i, ['fc3'])[0]\n",
    "print('logits', o)\n",
    "m = nn.Softmax(dim=1)\n",
    "pred = m(o)\n",
    "print('pred', pred)\n",
    "\n",
    "\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "vgg2 = models.vgg19()\n",
    "vgg2.features[0] = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "vgg2.avgpool = nn.AdaptiveAvgPool2d((8, 8))\n",
    "vgg2.classifier[0] = nn.Linear(512*8*8, 4096)\n",
    "vgg2.classifier[6] = nn.Linear(4096, 26)\n",
    "\n",
    "o2 = vgg2(i)\n",
    "print('logits', o2)\n",
    "pred2 = m(o2)\n",
    "print('pred', pred2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "train_dataset = datasets.MNIST(\n",
    "    './data',               # データの保存先\n",
    "    train = True,           # 学習用データを取得する\n",
    "    download = True,        # データが無い時にダウンロードする\n",
    "    transform = transform   # テンソルへの変換など\n",
    "    )\n",
    "# 評価用\n",
    "test_dataset = datasets.MNIST(\n",
    "    './data', \n",
    "    train = False,\n",
    "    transform = transform\n",
    "    )\n",
    "\n",
    "# データローダー\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = 128,\n",
    "    shuffle = True)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,     \n",
    "    batch_size = 128,\n",
    "    shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/tetta/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (conv1_1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=32768, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (fc3): Linear(in_features=4096, out_features=26, bias=True)\n",
      ")\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace=True)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vgg2 = VGG(26)\n",
    "vgg = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', init_weights=False)\n",
    "# vgg.features[0] = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "# vgg.avgpool = nn.AdaptiveAvgPool2d((8, 8))\n",
    "# vgg.classifier[0] = nn.Linear(512*8*8, 4096)\n",
    "# vgg.classifier[6] = nn.Linear(4096, 26)\n",
    "\n",
    "print(vgg2)\n",
    "print(vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ分割用CSV作成\n",
    "## 文字クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '../font2img'\n",
    "FONT_IMAGE_DIR = 'image'\n",
    "FONT_CSV = 'google_font_category_v4.csv'\n",
    "\n",
    "font_df = pd.read_csv(join(BASE_DIR, FONT_CSV))\n",
    "print(font_df.head())\n",
    "\n",
    "train_mask = font_df['data_type'] == 'train'\n",
    "valid_mask = font_df['data_type'] == 'valid'\n",
    "test_mask = ~(train_mask | valid_mask)\n",
    "\n",
    "train_df = font_df[train_mask]\n",
    "valid_df = font_df[valid_mask]\n",
    "test_df = font_df[test_mask]\n",
    "\n",
    "print('all ', len(font_df))\n",
    "print('train ', len(train_df))\n",
    "print('valid ', len(valid_df))\n",
    "print('test ', len(test_df))\n",
    "\n",
    "train_paths = []\n",
    "for _, line in train_df.iterrows():\n",
    "    train_paths += [line['font']+s for s in eval(line['subsets'])]\n",
    "\n",
    "valid_paths = []\n",
    "print(type(test_df))\n",
    "for _, line in valid_df.iterrows():\n",
    "    valid_paths += [line['font']+s for s in eval(line['subsets'])]\n",
    "\n",
    "test_paths = []\n",
    "for _, line in test_df.iterrows():\n",
    "    test_paths += [line['font']+s for s in eval(line['subsets'])]\n",
    "\n",
    "print(len(train_paths))\n",
    "print(len(valid_paths))\n",
    "print(len(test_paths))\n",
    "\n",
    "# style1_dir_list = glob(join(BASE_DIR, FONT_IMAGE_DIR, 'serif', '*', '*-Regular'))\n",
    "# print(style1_dir_list)\n",
    "\n",
    "# style2_dir_list = glob(join(BASE_DIR, FONT_IMAGE_DIR, 'sanserif', '*', '*-Regular'))\n",
    "# print(style1_dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(font_df[font_df['font'] == 'yujimai']['data_type'] == '-')\n",
    "if (font_df[font_df['font'] == 'yujimai']['data_type'] == '-').item():\n",
    "    print((font_df[font_df['font'] == 'yujimai']['data_type'] == 'f').item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = 'char_class.csv'\n",
    "\n",
    "font_dict = {'dirname': [], 'type': []}\n",
    "font_dir_path = [basename(p) for p in  glob(join(BASE_DIR, FONT_IMAGE_DIR, '*'))]\n",
    "empty_cnt = 0\n",
    "type_list = ['train', 'valid', 'test', 'train', 'test']\n",
    "for p in font_dir_path:\n",
    "    fontname = p.split('-')[0].split('[')[0].lower()\n",
    "    line = font_df[font_df['font'] == fontname]\n",
    "    if line.empty:\n",
    "        data_type = type_list[empty_cnt % len(type_list)]\n",
    "        empty_cnt += 1\n",
    "        font_dict['dirname'] += [p]\n",
    "        font_dict['type'] += [data_type]\n",
    "    else:\n",
    "        data_type = line['data_type'].item()\n",
    "        data_type = data_type if data_type != '-' else 'test'\n",
    "        font_dict['dirname'] += [p]\n",
    "        font_dict['type'] += [data_type]     \n",
    "\n",
    "data_path_df = pd.DataFrame(font_dict)\n",
    "data_path_df.to_csv(OUTPUT_FILE, index=False)\n",
    "# print(font_df.head())\n",
    "\n",
    "\n",
    "# t = [basename(p).split('-')[0].split('[')[0].lower() for p in glob(join(BASE_DIR, FONT_IMAGE_DIR, '*'))]\n",
    "# fontname = set(t)\n",
    "\n",
    "# for f in fontname:\n",
    "#     if f not in font_df['font'].values:\n",
    "#         print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_df = pd.read_csv(OUTPUT_FILE)\n",
    "\n",
    "train_path_df = data_path_df[data_path_df['type'] == 'train']\n",
    "valid_path_df = data_path_df[data_path_df['type'] == 'valid']\n",
    "test_path_df = data_path_df[data_path_df['type'] == 'test']\n",
    "\n",
    "print(len(data_path_df))\n",
    "print(len(train_path_df))\n",
    "print(len(valid_path_df))\n",
    "print(len(test_path_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文字スタイル用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0             font     category  \\\n",
      "0           0  ubuntucondensed   SANS_SERIF   \n",
      "1           1           ubuntu   SANS_SERIF   \n",
      "2           2       ubuntumono    MONOSPACE   \n",
      "3           3       robotomono    MONOSPACE   \n",
      "4           4      craftygirls  HANDWRITING   \n",
      "\n",
      "                                             subsets  isin_latin data_type  \n",
      "0  ['menu', 'cyrillic', 'cyrillic-ext', 'greek', ...        True     valid  \n",
      "1  ['cyrillic', 'cyrillic-ext', 'greek', 'greek-e...        True     valid  \n",
      "2  ['menu', 'cyrillic', 'cyrillic-ext', 'greek', ...        True     valid  \n",
      "3  ['cyrillic', 'cyrillic-ext', 'greek', 'latin',...        True     valid  \n",
      "4                                  ['latin', 'menu']        True     train  \n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = '../font2img'\n",
    "FONT_IMAGE_DIR = 'image'\n",
    "FONT_CSV = 'google_font_category_v4.csv'\n",
    "\n",
    "font_df = pd.read_csv(join(BASE_DIR, FONT_CSV))\n",
    "print(font_df.head())\n",
    "\n",
    "OUTPUT_FILE = 'char_style.csv'\n",
    "\n",
    "font_dict = {'dirname': [], 'type': [], 'style': []}\n",
    "font_dir_path = [basename(p) for p in  glob(join(BASE_DIR, FONT_IMAGE_DIR, '*'))]\n",
    "\n",
    "for p in font_dir_path:\n",
    "    fontname = p.split('-')[0].split('[')[0].lower()\n",
    "    line = font_df[font_df['font'] == fontname]\n",
    "    \n",
    "    if line.empty or line['category'].item() == 'MONOSPACE':\n",
    "        continue\n",
    "\n",
    "    data_type = line['data_type'].item()\n",
    "    data_type = data_type if data_type != '-' else 'test'\n",
    "    font_dict['dirname'] += [p]\n",
    "    font_dict['type'] += [data_type]\n",
    "    font_dict['style'] += [line['category'].item()]\n",
    "\n",
    "data_path_df = pd.DataFrame(font_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dirname</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>style</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DISPLAY</th>\n",
       "      <td>475</td>\n",
       "      <td>475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HANDWRITING</th>\n",
       "      <td>226</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SANS_SERIF</th>\n",
       "      <td>1341</td>\n",
       "      <td>1341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SERIF</th>\n",
       "      <td>613</td>\n",
       "      <td>613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dirname  type\n",
       "style                     \n",
       "DISPLAY          475   475\n",
       "HANDWRITING      226   226\n",
       "SANS_SERIF      1341  1341\n",
       "SERIF            613   613"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_df.groupby(['style']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dirname</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>style</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DISPLAY</th>\n",
       "      <td>260</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HANDWRITING</th>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SANS_SERIF</th>\n",
       "      <td>713</td>\n",
       "      <td>713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SERIF</th>\n",
       "      <td>301</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dirname  type\n",
       "style                     \n",
       "DISPLAY          260   260\n",
       "HANDWRITING      107   107\n",
       "SANS_SERIF       713   713\n",
       "SERIF            301   301"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_path_df[data_path_df['type'] == 'train'].groupby(['style']).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dirname</th>\n",
       "      <th>type</th>\n",
       "      <th>style</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YujiSyuku-Regular</td>\n",
       "      <td>test</td>\n",
       "      <td>SERIF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Taviraj-Regular</td>\n",
       "      <td>valid</td>\n",
       "      <td>SERIF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OvertheRainbow</td>\n",
       "      <td>test</td>\n",
       "      <td>HANDWRITING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BeVietnamPro-ExtraLightItalic</td>\n",
       "      <td>test</td>\n",
       "      <td>SANS_SERIF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BaiJamjuree-SemiBold</td>\n",
       "      <td>test</td>\n",
       "      <td>SANS_SERIF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381</th>\n",
       "      <td>Maitree-Medium</td>\n",
       "      <td>train</td>\n",
       "      <td>SERIF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>BioRhyme-Light</td>\n",
       "      <td>train</td>\n",
       "      <td>SERIF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>Gurajada-Regular</td>\n",
       "      <td>train</td>\n",
       "      <td>SERIF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>FaunaOne-Regular</td>\n",
       "      <td>train</td>\n",
       "      <td>SERIF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>Faustina[wght]</td>\n",
       "      <td>train</td>\n",
       "      <td>SERIF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1702 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            dirname   type        style\n",
       "1                 YujiSyuku-Regular   test        SERIF\n",
       "2                   Taviraj-Regular  valid        SERIF\n",
       "4                    OvertheRainbow   test  HANDWRITING\n",
       "5     BeVietnamPro-ExtraLightItalic   test   SANS_SERIF\n",
       "7              BaiJamjuree-SemiBold   test   SANS_SERIF\n",
       "...                             ...    ...          ...\n",
       "2381                 Maitree-Medium  train        SERIF\n",
       "86                   BioRhyme-Light  train        SERIF\n",
       "406                Gurajada-Regular  train        SERIF\n",
       "1115               FaunaOne-Regular  train        SERIF\n",
       "1503                 Faustina[wght]  train        SERIF\n",
       "\n",
       "[1702 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path_df = data_path_df[data_path_df['type'] == 'train']\n",
    "data_path_df = data_path_df[data_path_df['type'] != 'train']\n",
    "\n",
    "# each style size is 107 \n",
    "display_data_path_df = train_data_path_df[train_data_path_df['style'] == 'DISPLAY'].sample(n=107)\n",
    "hw_data_path_df = train_data_path_df[train_data_path_df['style'] == 'HANDWRITING'].sample(n=107)\n",
    "sanserif_data_path_df = train_data_path_df[train_data_path_df['style'] == 'SANS_SERIF'].sample(n=107)\n",
    "serif_display_data_path_df = train_data_path_df[train_data_path_df['style'] == 'SERIF'].sample(n=107)\n",
    "\n",
    "new_train_data_path_df = pd.concat([display_data_path_df, hw_data_path_df, sanserif_data_path_df, serif_display_data_path_df])\n",
    "\n",
    "data_path_df = pd.concat([data_path_df, new_train_data_path_df])\n",
    "\n",
    "data_path_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1702\n",
      "428\n",
      "448\n",
      "826\n"
     ]
    }
   ],
   "source": [
    "data_path_df.to_csv(OUTPUT_FILE, index = False)\n",
    "\n",
    "data_path_df = pd.read_csv(OUTPUT_FILE)\n",
    "\n",
    "train_path_df = data_path_df[data_path_df['type'] == 'train']\n",
    "valid_path_df = data_path_df[data_path_df['type'] == 'valid']\n",
    "test_path_df = data_path_df[data_path_df['type'] == 'test']\n",
    "\n",
    "print(len(data_path_df))\n",
    "print(len(train_path_df))\n",
    "print(len(valid_path_df))\n",
    "print(len(test_path_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dirname</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>style</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DISPLAY</th>\n",
       "      <td>322</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HANDWRITING</th>\n",
       "      <td>226</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SANS_SERIF</th>\n",
       "      <td>735</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SERIF</th>\n",
       "      <td>419</td>\n",
       "      <td>419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dirname  type\n",
       "style                     \n",
       "DISPLAY          322   322\n",
       "HANDWRITING      226   226\n",
       "SANS_SERIF       735   735\n",
       "SERIF            419   419"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check agein\n",
    "data_path_df.groupby(['style']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dirname</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>style</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DISPLAY</th>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HANDWRITING</th>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SANS_SERIF</th>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SERIF</th>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dirname  type\n",
       "style                     \n",
       "DISPLAY          107   107\n",
       "HANDWRITING      107   107\n",
       "SANS_SERIF       107   107\n",
       "SERIF            107   107"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train data をさいどかくにん\n",
    "data_path_df[data_path_df['type'] == 'train'].groupby(['style']).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.ones(1, 3, 4, 4)\n",
    "b,c,h,w = i.size()\n",
    "Fe = i.view(b, c, h*w)\n",
    "print('Fe shape: ', Fe.shape)\n",
    "print('Fe: ', Fe)\n",
    "print('Fe trans shape: ', Fe.transpose(1,2).shape)\n",
    "G = torch.bmm(Fe, Fe.transpose(1,2))\n",
    "print('G shape: ', G.shape)\n",
    "print('G before div: ', G)\n",
    "G = G.div(c*h*w)\n",
    "print('G after div: ', G)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.ones(1,3,4,4)\n",
    "\n",
    "a, b, c, d = input.size()  # a=batch size(=1)\n",
    "# b=number of feature maps\n",
    "# (c,d)=dimensions of a f. map (N=c*d)\n",
    "\n",
    "features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n",
    "print('features shape: ', features.shape)\n",
    "print('features: ', features)\n",
    "\n",
    "G = torch.mm(features, features.t())  # compute the gram product\n",
    "\n",
    "print('G shape: ', G.shape)\n",
    "print('G: ', G)\n",
    "print('G div: ', G.div(a*b*c*d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "a += [1]\n",
    "print(a)\n",
    "a += [2,3,4,5,6,7]\n",
    "print(a[::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "print(torch.tensor(mean).view(-1, 1, 1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save VGG imagenet weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "vgg = models.vgg19(weights='IMAGENET1K_V1').features\n",
    "\n",
    "# vgg.features[0] = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "# print(vgg)\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # normalize img\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "device = 'cuda:0'\n",
    "cnn_normalization_mean = [0.48235, 0.45882, 0.40784]\n",
    "cnn_normalization_std = [1/255, 1/255, 1/255]\n",
    "\n",
    "normalization = Normalization(cnn_normalization_mean, cnn_normalization_std).to(device)\n",
    "model = nn.Sequential(normalization)\n",
    "# class VGG(nn.Module):\n",
    "#     def __init__(self, pool='max'):\n",
    "#         super(VGG, self).__init__()\n",
    "# layers = OrderedDict()\n",
    "# layers['prep'] = normalization\n",
    "conv_i = 1  # increment every time we see a conv\n",
    "pool_i = 1\n",
    "\n",
    "for layer in vgg.children():\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        name = 'conv{}_{}'.format(pool_i, conv_i)\n",
    "        conv_i += 1\n",
    "    elif isinstance(layer, nn.ReLU):\n",
    "        continue\n",
    "        # name = 'relu_{}'.format(i)\n",
    "        # # The in-place version doesn't play very nicely with the ContentLoss\n",
    "        # # and StyleLoss we insert below. So we replace with out-of-place\n",
    "        # # ones here.\n",
    "        # layer = nn.ReLU(inplace=False)\n",
    "    elif isinstance(layer, nn.MaxPool2d):\n",
    "        name = 'pool{}'.format(pool_i)\n",
    "        conv_i = 1\n",
    "        pool_i += 1\n",
    "    else:\n",
    "        raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "    \n",
    "    # layers[name] = layer\n",
    "    model.add_module(name, layer)\n",
    "    \n",
    "model.to(device)\n",
    "torch.save(model.state_dict(), 'vgg_imagenet_0_1.pth')\n",
    "    \n",
    "# for k, v in layers.items():\n",
    "#     print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model_weights = torch.load('vgg_imagenet_0_1.pth', map_location={'cuda': 'cpu'})\n",
    "\n",
    "from utility.vgg_network import VGG\n",
    "\n",
    "vgg = VGG()\n",
    "vgg.load_state_dict(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (conv1_1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(8, 8))\n",
      "  (fc1): Linear(in_features=32768, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (fc3): Linear(in_features=4096, out_features=26, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from utility.vgg_network_with_top import VGG\n",
    "v = VGG(26)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save AlexNet imagenet weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "alexnet = models.alexnet(weights='IMAGENET1K_V1').features\n",
    "\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # normalize img\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "device = 'cuda:0'\n",
    "cnn_normalization_mean = [0.485, 0.456, 0.406]\n",
    "cnn_normalization_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "normalization = Normalization(cnn_normalization_mean, cnn_normalization_std).to(device)\n",
    "model = nn.Sequential(normalization)\n",
    "\n",
    "conv_cnt = 0# increment every time we see a conv\n",
    "dropout_cnt = 0\n",
    "linear_cnt = 0\n",
    "\n",
    "for layer in alexnet.children():\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        conv_cnt += 1\n",
    "        name = 'conv{}'.format(conv_cnt)\n",
    "    elif isinstance(layer, nn.ReLU):\n",
    "        continue\n",
    "        # name = 'relu_{}'.format(conv_cnt)\n",
    "        # # The in-place version doesn't play very nicely with the ContentLoss\n",
    "        # # and StyleLoss we insert below. So we replace with out-of-place\n",
    "        # # ones here.\n",
    "        # layer = nn.ReLU(inplace=False)\n",
    "    elif isinstance(layer, nn.MaxPool2d):\n",
    "        name = 'pool{}'.format(conv_cnt)\n",
    "    elif isinstance(layer, nn.AdaptiveAvgPool2d):\n",
    "        name = 'avgpool'\n",
    "    elif isinstance(layer, nn.Dropout):\n",
    "        dropout_cnt += 1\n",
    "        name = 'dropout{}'.format(dropout_cnt)\n",
    "    elif isinstance(layer, nn.Linear):\n",
    "        linear_cnt += 1\n",
    "        name = 'fc{}'.format(linear_cnt)\n",
    "    \n",
    "    else:\n",
    "        raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "    \n",
    "    # layers[name] = layer\n",
    "    model.add_module(name, layer)\n",
    "\n",
    "model.to(device)\n",
    "torch.save(model.state_dict(), 'alexnet_imagenet_0_1.pth')\n",
    "    \n",
    "# for k, v in layers.items():\n",
    "#     print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tetta/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model_weights = torch.load('alexnet_imagenet_0_1.pth', map_location={'cuda': 'cpu'})\n",
    "\n",
    "from utility.alexnet import AlexNet\n",
    "\n",
    "alexnet = AlexNet(26)\n",
    "alexnet.load_state_dict(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=26, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "i = torch.randn(8, 1, 256, 256)\n",
    "vgg = models.resnet18(pretrained=False)\n",
    "vgg.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "vgg.fc = nn.Linear(512,26)\n",
    "o = vgg(i)\n",
    "print(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tetta/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SIZE:  41990\n",
      "VAL SIZE:  13624\n",
      "TEST SIZE:  25714\n"
     ]
    }
   ],
   "source": [
    "from train_vgg import make_dataloader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "d = make_dataloader('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(151.0610)\n",
      "tensor(-123.6800)\n"
     ]
    }
   ],
   "source": [
    "print(d['train'].dataset[0][0].max())\n",
    "print(d['train'].dataset[0][0].min())\n",
    "\n",
    "# plt.imshow(d['train'].dataset[0][0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tetta/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/tetta/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SIZE:  41990\n",
      "VAL SIZE:  13624\n",
      "TEST SIZE:  25714\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from collections import OrderedDict\n",
    "\n",
    "vgg = models.resnet18(pretrained=False)\n",
    "vgg.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "vgg.fc = nn.Linear(512, 26)\n",
    "\n",
    "\n",
    "model_path = 'resnet18.pth'\n",
    "model_weights = torch.load(model_path, map_location={'cuda': 'cpu'})\n",
    "\n",
    "vgg.load_state_dict(model_weights)\n",
    "\n",
    "from train_vgg import make_dataloader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "d = make_dataloader('cpu')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=26, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "vgg.eval()\n",
    "\n",
    "from utility.utility import load_mono_images\n",
    "\n",
    "f = '../font2img/image/Abel-Regular/D.png'\n",
    "i = load_mono_images(f, 256, 'cpu', 1)\n",
    "i = i.unsqueeze(0)\n",
    "o = vgg(i)\n",
    "# print(i[0][0][100:120, 100:120])\n",
    "print(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "bn1 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "relu ReLU(inplace=True)\n",
      "maxpool MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "layer1 Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "layer2 Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "layer3 Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "layer4 Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "avgpool AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "fc Linear(in_features=512, out_features=26, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "\n",
    "\n",
    "for n, m in vgg.named_children():\n",
    "    print(n, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 26])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utility.alexnet import AlexNet\n",
    "\n",
    "model = AlexNet(26)\n",
    "i = torch.zeros(16, 3, 256, 256)\n",
    "\n",
    "o = model(i, ['fc3'])\n",
    "print(o[0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('research-vbjgNxHm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82f3adc9fbda2fc26498424c39d81eed29eee43e13f5d50b10cf9154c73bd2c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
