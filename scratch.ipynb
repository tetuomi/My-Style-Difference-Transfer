{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from os.path import basename\n",
    "from os.path import join\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5791e-09, 2.1640e-08, 2.5100e-08, 6.6962e-09, 3.2617e-07, 1.8972e-11,\n",
      "         7.3669e-08, 1.9988e-08, 4.7147e-03, 9.0731e-08, 3.3016e-08, 9.9518e-01,\n",
      "         8.4838e-13, 1.5790e-09, 5.2730e-12, 5.4941e-11, 6.6949e-08, 1.9095e-09,\n",
      "         2.8374e-08, 1.0302e-09, 2.6120e-09, 2.1241e-09, 6.2753e-14, 3.7070e-09,\n",
      "         5.2187e-08, 1.0815e-04]], device='cuda:0')\n",
      "class L\n",
      "tensor(20.2664, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from utility.utility import load_images\n",
    "from utility.vgg_network_with_top import VGG as VGGWithTOP\n",
    "\n",
    "f = 'output/265/[1020].jpg'\n",
    "img = load_images(f, 256, 'cuda:0', 1)\n",
    "# print(img)\n",
    "# vgg_with_top = VGGWithTOP()\n",
    "# vgg_with_top.load_state_dict(torch.load('./vgg_with_top.pth'))\n",
    "from torchvision import models\n",
    "vgg_with_top = models.resnet18(weights=None)\n",
    "vgg_with_top.fc = nn.Linear(512, 26)\n",
    "vgg_with_top.load_state_dict(torch.load('./resnet_0_1.pth'))\n",
    "\n",
    "for param in vgg_with_top.parameters():\n",
    "    param.requires_grad = False\n",
    "vgg_with_top.to('cuda:0')\n",
    "vgg_with_top.eval()\n",
    "\n",
    "content_class = 'A'\n",
    "label = torch.tensor([i for i in range(26) if chr(i+ord('A')) == content_class]).to('cuda:0')\n",
    "# logit = vgg_with_top(img, ['fc3'])[0]\n",
    "logit = vgg_with_top(img)\n",
    "# print(logit)\n",
    "prob = nn.Softmax(dim=1)(logit)\n",
    "print(prob)\n",
    "pred_label = torch.max(prob, 1)[1].item()\n",
    "print('class', chr(ord('A') + pred_label))\n",
    "loss = nn.CrossEntropyLoss()(logit, label)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SIZE:  41990\n",
      "VAL SIZE:  13624\n",
      "TEST SIZE:  25714\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1608, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5412, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0157, 0.9059, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.3020, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.6863, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0824, 0.9804, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.4431, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.8196, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1961,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5725,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 0.9725],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0196, 0.9216,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 0.6667],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3216, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 0.2980],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6980, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         0.9137, 0.0157],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0863, 0.9843, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         0.5608, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4471, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         0.1882, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8196, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8196,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1961, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.4510,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5647, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9882, 0.0941,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.9098, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7137, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2863, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.3412, 0.0000,\n",
      "         0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "from train_vgg import make_dataloader\n",
    "\n",
    "d = make_dataloader()\n",
    "print(d['test'].dataset[0][0][0][100:120,100:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SIZE:  41990\n",
      "VAL SIZE:  13624\n",
      "TEST SIZE:  25714\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000001?line=4'>5</a>\u001b[0m d \u001b[39m=\u001b[39m make_dataloader(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000001?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize \u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000001?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, l \u001b[39min\u001b[39;00m d[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000001?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _i, _l \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(i, l): \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000001?line=8'>9</a>\u001b[0m         plt\u001b[39m.\u001b[39msubplot(\u001b[39m4\u001b[39m, \u001b[39m7\u001b[39m, _l\u001b[39m.\u001b[39mitem()\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torch/utils/data/dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    691\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    694\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/works/research/Style-Difference-Transfer/train_vgg.py:58\u001b[0m, in \u001b[0;36mLoadDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     56\u001b[0m filepath \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_list[index]\n\u001b[1;32m     57\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(filepath)\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m img_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m     59\u001b[0m alphabet \u001b[39m=\u001b[39m path\u001b[39m.\u001b[39mbasename(filepath)[:\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m]\n\u001b[1;32m     60\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mord\u001b[39m(alphabet) \u001b[39m-\u001b[39m \u001b[39mord\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from train_vgg import make_path_list, make_dataloader\n",
    "\n",
    "# t, _, _ = make_path_list()\n",
    "# print(t[:10])\n",
    "d = make_dataloader('cpu')\n",
    "plt.figure(figsize =(10, 10))\n",
    "for i, l in d['train']:\n",
    "    for _i, _l in zip(i, l): \n",
    "        plt.subplot(4, 7, _l.item()+1)\n",
    "        plt.imshow(_i[0], cmap='gray')\n",
    "\n",
    "plt.figure(figsize =(10, 10))\n",
    "for i, l in d['val']:\n",
    "    for _i, _l in zip(i, l): \n",
    "        plt.subplot(4, 7, _l.item()+1)\n",
    "        plt.imshow(_i[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SIZE:  41990\n",
      "VAL SIZE:  13624\n",
      "TEST SIZE:  25714\n",
      "i tensor([[[[-0.5423,  0.3065, -0.3376,  ..., -0.4974,  0.8768, -0.1035],\n",
      "          [ 0.5822, -1.0294,  0.9340,  ..., -0.1026, -0.9206, -0.1713],\n",
      "          [ 0.6879, -2.0340,  0.6456,  ..., -0.3766, -0.7715,  0.8524],\n",
      "          ...,\n",
      "          [ 1.7260,  1.2476, -0.6634,  ..., -2.3865,  0.3136,  0.0300],\n",
      "          [-0.4887, -0.4332,  0.0106,  ...,  0.6427, -0.3805, -0.1196],\n",
      "          [-0.0113,  0.9679, -2.3826,  ..., -1.8271,  0.2571,  0.6796]]],\n",
      "\n",
      "\n",
      "        [[[-0.5027, -1.1181,  1.0078,  ..., -0.8235,  0.5323, -1.1458],\n",
      "          [-1.0572,  1.1472,  0.2468,  ...,  1.1336,  0.1664, -0.9696],\n",
      "          [-0.9418, -0.1571, -0.3978,  ...,  1.3375,  0.2424,  0.0682],\n",
      "          ...,\n",
      "          [-0.9631, -1.0917,  1.7891,  ..., -1.3390,  0.6551,  1.1878],\n",
      "          [-0.2230,  0.2640, -0.2684,  ..., -0.8585, -0.1678, -1.2309],\n",
      "          [-1.1352,  0.5258,  0.1004,  ...,  2.0580,  0.0970, -0.8208]]],\n",
      "\n",
      "\n",
      "        [[[-1.6625,  0.9490,  1.8656,  ..., -1.8982,  1.7411,  0.3963],\n",
      "          [-0.7133, -1.4019,  1.1369,  ..., -0.9975,  1.1331, -0.7550],\n",
      "          [-0.0642,  0.3218, -0.0917,  ...,  0.5686, -0.5694,  1.1757],\n",
      "          ...,\n",
      "          [-0.1513, -0.2195, -0.1939,  ...,  1.9498, -0.9619,  0.6784],\n",
      "          [ 0.6543,  0.6269,  0.0578,  ...,  0.2402,  0.3461,  0.3772],\n",
      "          [-1.1519, -1.7594,  0.2500,  ...,  1.5730,  0.2474, -0.7736]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.8287, -0.4754, -0.1954,  ...,  1.0040,  0.5933, -0.5574],\n",
      "          [ 1.4506, -0.5111, -0.1198,  ..., -0.2150,  0.1655,  0.2571],\n",
      "          [-0.6803, -0.3366,  0.1426,  ..., -0.6580,  0.3054,  0.8554],\n",
      "          ...,\n",
      "          [-0.3221, -0.8873,  0.0183,  ...,  0.5928,  1.7622,  0.7421],\n",
      "          [-0.4659,  0.4088, -0.9953,  ..., -0.3493, -2.6852,  0.3789],\n",
      "          [ 0.2998, -1.0955,  0.8505,  ...,  0.0906,  1.2650,  1.5996]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3821, -0.1671,  0.5031,  ...,  0.2560, -0.3141, -0.2441],\n",
      "          [-0.3634, -0.3271, -1.5936,  ...,  0.0592, -1.2590,  0.2881],\n",
      "          [ 0.5435,  0.6737, -1.0612,  ...,  2.7391,  0.5299, -2.0136],\n",
      "          ...,\n",
      "          [ 1.5568,  0.3187, -0.6020,  ...,  1.1930,  0.5432,  0.4547],\n",
      "          [ 0.6651,  0.8410, -0.0999,  ...,  0.9296,  0.1281,  1.6398],\n",
      "          [-0.4841, -0.1612,  0.2045,  ...,  0.1016, -1.2534,  1.0817]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5120,  1.0903, -0.2918,  ...,  1.2709, -0.0378,  0.3350],\n",
      "          [-0.0050, -0.3147,  1.5113,  ..., -2.2863, -0.6311, -0.7261],\n",
      "          [ 1.7034,  0.0785, -0.1993,  ...,  0.5745, -1.1855, -0.6811],\n",
      "          ...,\n",
      "          [ 0.0909,  0.1733,  0.0605,  ...,  1.1779,  1.5189,  0.4303],\n",
      "          [-0.5361, -0.5759, -0.1050,  ..., -1.9115, -1.5877, -0.1922],\n",
      "          [-0.0835,  0.6974,  1.5129,  ...,  0.2917,  0.3700, -0.2230]]]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 3, 3], expected input[10, 1, 256, 256] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000001?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m'\u001b[39m, i)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000001?line=19'>20</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000001?line=20'>21</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mout1\u001b[39m\u001b[39m'\u001b[39m, vgg(i, [\u001b[39m'\u001b[39;49m\u001b[39mfc3\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000001?line=21'>22</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mout2\u001b[39m\u001b[39m'\u001b[39m, vgg2(i))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000001?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, l \u001b[39min\u001b[39;00m d[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/works/research/Style-Difference-Transfer/utility/vgg_network_with_top.py:63\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, x, out_keys)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, out_keys):\n\u001b[1;32m     62\u001b[0m     out \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> 63\u001b[0m     out[\u001b[39m'\u001b[39m\u001b[39mr11\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1_1(x))\n\u001b[1;32m     64\u001b[0m     out[\u001b[39m'\u001b[39m\u001b[39mr12\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1_2(out[\u001b[39m'\u001b[39m\u001b[39mr11\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m     65\u001b[0m     out[\u001b[39m'\u001b[39m\u001b[39mp1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool1(out[\u001b[39m'\u001b[39m\u001b[39mr12\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 3, 3], expected input[10, 1, 256, 256] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "from utility.vgg_network_with_top import VGG, Model\n",
    "from train_vgg import make_path_list, make_dataloader\n",
    "\n",
    "d = make_dataloader('cpu')\n",
    "vgg = VGG(26)\n",
    "vgg.eval()\n",
    "\n",
    "# vgg2 = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', init_weights=False)\n",
    "# vgg2.features[0] = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "# vgg2.avgpool = nn.AdaptiveAvgPool2d((8, 8))\n",
    "# vgg2.classifier[0] = nn.Linear(512*8*8, 4096)\n",
    "# vgg2.classifier[6] = nn.Linear(4096, 26)\n",
    "# vgg2.eval()\n",
    "\n",
    "# m = Model()\n",
    "# m.eval()\n",
    "\n",
    "i = torch.randn(10, 1, 256, 256)\n",
    "print('i', i)\n",
    "with torch.no_grad():\n",
    "    print('out1', vgg(i, ['fc3']))\n",
    "    print('out2', vgg2(i))\n",
    "\n",
    "for i, l in d['train']:\n",
    "    o1 = vgg(i, ['fc3'])\n",
    "    # o2 = vgg2(i)\n",
    "    break\n",
    "\n",
    "print(o1)\n",
    "# print(o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.unsqueeze(torch.tensor(0), dim=0)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-123.6800)\n",
      "tensor(151.0610)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "img should be Tensor Image. Got <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000002?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(i\u001b[39m.\u001b[39mmax())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000002?line=26'>27</a>\u001b[0m i \u001b[39m=\u001b[39m max_min_scale(i)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000002?line=27'>28</a>\u001b[0m prep(i)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000002?line=28'>29</a>\u001b[0m \u001b[39mprint\u001b[39m(i\u001b[39m.\u001b[39mmin())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tetta/works/research/Style-Difference-Transfer/scratch.ipynb#ch0000002?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(i\u001b[39m.\u001b[39mmax())\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torchvision/transforms/transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 94\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torchvision/transforms/transforms.py:269\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    262\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torchvision/transforms/functional.py:358\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    356\u001b[0m     _log_api_usage_once(normalize)\n\u001b[1;32m    357\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 358\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be Tensor Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    360\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mnormalize(tensor, mean\u001b[39m=\u001b[39mmean, std\u001b[39m=\u001b[39mstd, inplace\u001b[39m=\u001b[39minplace)\n",
      "\u001b[0;31mTypeError\u001b[0m: img should be Tensor Image. Got <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from utility.utility import load_images, max_min_scale\n",
    "\n",
    "f = '../font2img/image/Abel-Regular/D.png'\n",
    "# f = '/home/tetta/Pictures/tetta_kondo.JPG'\n",
    "i = load_images(f, 256, 'cpu', 1)\n",
    "\n",
    "# print(i.shape)\n",
    "# print(i[0][0][0:5, 0:5])\n",
    "# print(i[0][0][100:120, 80:90])\n",
    "# plt.imshow(i[0].permute(1,2,0))\n",
    "# print(i.dtype)\n",
    "\n",
    "prep = transforms.Compose([\n",
    "                        # transforms.Resize((256,256)),\n",
    "                        # transforms.RandomRotation(angle),\n",
    "                        # transforms.ToTensor(),\n",
    "                        # transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to BGR\n",
    "                        transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], #subtract imagenet mean\n",
    "                                                std=[1,1,1]),\n",
    "                        transforms.Lambda(lambda x: x.mul_(255)),\n",
    "                        ])\n",
    "\n",
    "print(i.min())\n",
    "print(i.max())\n",
    "\n",
    "# i = max_min_scale(i)\n",
    "prep(i)\n",
    "print(i.min())\n",
    "print(i.max())\n",
    "# plt.imshow(i[0].permute(1,2,0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits tensor([[-0.1058,  0.1741, -0.0923, -0.0151, -0.2229, -0.2300, -0.0282, -0.0360,\n",
      "          0.0891,  0.0046,  0.2404, -0.0240, -0.0099,  0.0192,  0.0455,  0.1102,\n",
      "         -0.0802,  0.0678, -0.0605, -0.0148,  0.0418, -0.0833, -0.0018,  0.1328,\n",
      "          0.0070, -0.1701],\n",
      "        [-0.1210,  0.0962,  0.0478, -0.1091, -0.1825, -0.2050, -0.0410, -0.0358,\n",
      "         -0.0169, -0.1034,  0.1817,  0.0946, -0.0660, -0.1040, -0.1219, -0.0929,\n",
      "         -0.0100,  0.1631, -0.1130, -0.0129,  0.1019, -0.0275, -0.1003, -0.0932,\n",
      "          0.0661, -0.1777]], grad_fn=<AddmmBackward0>)\n",
      "pred tensor([[0.0347, 0.0459, 0.0352, 0.0380, 0.0309, 0.0307, 0.0375, 0.0372, 0.0422,\n",
      "         0.0388, 0.0491, 0.0377, 0.0382, 0.0393, 0.0404, 0.0431, 0.0356, 0.0413,\n",
      "         0.0363, 0.0380, 0.0402, 0.0355, 0.0385, 0.0441, 0.0389, 0.0326],\n",
      "        [0.0352, 0.0437, 0.0417, 0.0356, 0.0331, 0.0324, 0.0381, 0.0383, 0.0391,\n",
      "         0.0358, 0.0476, 0.0437, 0.0372, 0.0358, 0.0352, 0.0362, 0.0393, 0.0468,\n",
      "         0.0355, 0.0392, 0.0440, 0.0387, 0.0359, 0.0362, 0.0424, 0.0333]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "logits tensor([[-0.0348,  0.0225, -0.0269, -0.0461,  0.0233,  0.0748, -0.0116, -0.0346,\n",
      "          0.1197,  0.0875, -0.0025, -0.1038,  0.0362, -0.1062,  0.0056,  0.1152,\n",
      "         -0.1096,  0.0059, -0.0523,  0.0188,  0.0035, -0.0186, -0.0559, -0.0624,\n",
      "         -0.0650, -0.1030],\n",
      "        [ 0.0552, -0.0111, -0.0155, -0.1681, -0.1512,  0.1471, -0.0802,  0.0462,\n",
      "          0.2316,  0.1131, -0.0462, -0.0679, -0.0467, -0.0067,  0.0757,  0.1023,\n",
      "          0.0061, -0.1386, -0.0464, -0.0758, -0.0348,  0.0315, -0.1796,  0.0482,\n",
      "          0.0512, -0.1800]], grad_fn=<AddmmBackward0>)\n",
      "pred tensor([[0.0375, 0.0397, 0.0378, 0.0371, 0.0398, 0.0419, 0.0384, 0.0375, 0.0438,\n",
      "         0.0424, 0.0388, 0.0350, 0.0403, 0.0349, 0.0391, 0.0436, 0.0348, 0.0391,\n",
      "         0.0369, 0.0396, 0.0390, 0.0381, 0.0367, 0.0365, 0.0364, 0.0351],\n",
      "        [0.0410, 0.0383, 0.0382, 0.0328, 0.0333, 0.0449, 0.0358, 0.0406, 0.0489,\n",
      "         0.0434, 0.0370, 0.0362, 0.0370, 0.0385, 0.0418, 0.0429, 0.0390, 0.0337,\n",
      "         0.0370, 0.0359, 0.0374, 0.0400, 0.0324, 0.0407, 0.0408, 0.0324]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from utility.vgg_network_with_top import VGG\n",
    "import torch\n",
    "\n",
    "vgg = VGG(26)\n",
    "# i = torch.ones(12, 1, 256, 256)\n",
    "i = torch.randn(2, 1, 256, 256)\n",
    "o = vgg(i, ['fc3'])[0]\n",
    "print('logits', o)\n",
    "m = nn.Softmax(dim=1)\n",
    "pred = m(o)\n",
    "print('pred', pred)\n",
    "\n",
    "\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "vgg2 = models.vgg19()\n",
    "vgg2.features[0] = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "vgg2.avgpool = nn.AdaptiveAvgPool2d((8, 8))\n",
    "vgg2.classifier[0] = nn.Linear(512*8*8, 4096)\n",
    "vgg2.classifier[6] = nn.Linear(4096, 26)\n",
    "\n",
    "o2 = vgg2(i)\n",
    "print('logits', o2)\n",
    "pred2 = m(o2)\n",
    "print('pred', pred2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "train_dataset = datasets.MNIST(\n",
    "    './data',               # データの保存先\n",
    "    train = True,           # 学習用データを取得する\n",
    "    download = True,        # データが無い時にダウンロードする\n",
    "    transform = transform   # テンソルへの変換など\n",
    "    )\n",
    "# 評価用\n",
    "test_dataset = datasets.MNIST(\n",
    "    './data', \n",
    "    train = False,\n",
    "    transform = transform\n",
    "    )\n",
    "\n",
    "# データローダー\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = 128,\n",
    "    shuffle = True)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,     \n",
    "    batch_size = 128,\n",
    "    shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/tetta/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (conv1_1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=32768, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (fc3): Linear(in_features=4096, out_features=26, bias=True)\n",
      ")\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace=True)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vgg2 = VGG(26)\n",
    "vgg = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', init_weights=False)\n",
    "# vgg.features[0] = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "# vgg.avgpool = nn.AdaptiveAvgPool2d((8, 8))\n",
    "# vgg.classifier[0] = nn.Linear(512*8*8, 4096)\n",
    "# vgg.classifier[6] = nn.Linear(4096, 26)\n",
    "\n",
    "print(vgg2)\n",
    "print(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '../font2img'\n",
    "FONT_IMAGE_DIR = 'image'\n",
    "FONT_CSV = 'google_font_category_v4.csv'\n",
    "\n",
    "font_df = pd.read_csv(join(BASE_DIR, FONT_CSV))\n",
    "print(font_df.head())\n",
    "\n",
    "train_mask = font_df['data_type'] == 'train'\n",
    "valid_mask = font_df['data_type'] == 'valid'\n",
    "test_mask = ~(train_mask | valid_mask)\n",
    "\n",
    "train_df = font_df[train_mask]\n",
    "valid_df = font_df[valid_mask]\n",
    "test_df = font_df[test_mask]\n",
    "\n",
    "print('all ', len(font_df))\n",
    "print('train ', len(train_df))\n",
    "print('valid ', len(valid_df))\n",
    "print('test ', len(test_df))\n",
    "\n",
    "train_paths = []\n",
    "for _, line in train_df.iterrows():\n",
    "    train_paths += [line['font']+s for s in eval(line['subsets'])]\n",
    "\n",
    "valid_paths = []\n",
    "print(type(test_df))\n",
    "for _, line in valid_df.iterrows():\n",
    "    valid_paths += [line['font']+s for s in eval(line['subsets'])]\n",
    "\n",
    "test_paths = []\n",
    "for _, line in test_df.iterrows():\n",
    "    test_paths += [line['font']+s for s in eval(line['subsets'])]\n",
    "\n",
    "print(len(train_paths))\n",
    "print(len(valid_paths))\n",
    "print(len(test_paths))\n",
    "\n",
    "# style1_dir_list = glob(join(BASE_DIR, FONT_IMAGE_DIR, 'serif', '*', '*-Regular'))\n",
    "# print(style1_dir_list)\n",
    "\n",
    "# style2_dir_list = glob(join(BASE_DIR, FONT_IMAGE_DIR, 'sanserif', '*', '*-Regular'))\n",
    "# print(style1_dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(font_df[font_df['font'] == 'yujimai']['data_type'] == '-')\n",
    "if (font_df[font_df['font'] == 'yujimai']['data_type'] == '-').item():\n",
    "    print((font_df[font_df['font'] == 'yujimai']['data_type'] == 'f').item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = 'data.csv'\n",
    "\n",
    "font_dict = {'dirname': [], 'type': []}\n",
    "font_dir_path = [basename(p) for p in  glob(join(BASE_DIR, FONT_IMAGE_DIR, '*'))]\n",
    "empty_cnt = 0\n",
    "type_list = ['train', 'valid', 'test', 'train', 'test']\n",
    "for p in font_dir_path:\n",
    "    fontname = p.split('-')[0].split('[')[0].lower()\n",
    "    line = font_df[font_df['font'] == fontname]\n",
    "    if line.empty:\n",
    "        data_type = type_list[empty_cnt % len(type_list)]\n",
    "        empty_cnt += 1\n",
    "        font_dict['dirname'] += [p]\n",
    "        font_dict['type'] += [data_type]\n",
    "    else:\n",
    "        data_type = line['data_type'].item()\n",
    "        data_type = data_type if data_type != '-' else 'test'\n",
    "        font_dict['dirname'] += [p]\n",
    "        font_dict['type'] += [data_type]     \n",
    "\n",
    "data_path_df = pd.DataFrame(font_dict)\n",
    "data_path_df.to_csv(OUTPUT_FILE, index=False)\n",
    "# print(font_df.head())\n",
    "\n",
    "\n",
    "# t = [basename(p).split('-')[0].split('[')[0].lower() for p in glob(join(BASE_DIR, FONT_IMAGE_DIR, '*'))]\n",
    "# fontname = set(t)\n",
    "\n",
    "# for f in fontname:\n",
    "#     if f not in font_df['font'].values:\n",
    "#         print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_df = pd.read_csv(OUTPUT_FILE)\n",
    "\n",
    "train_path_df = data_path_df[data_path_df['type'] == 'train']\n",
    "valid_path_df = data_path_df[data_path_df['type'] == 'valid']\n",
    "test_path_df = data_path_df[data_path_df['type'] == 'test']\n",
    "\n",
    "print(len(data_path_df))\n",
    "print(len(train_path_df))\n",
    "print(len(valid_path_df))\n",
    "print(len(test_path_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.ones(1, 3, 4, 4)\n",
    "b,c,h,w = i.size()\n",
    "Fe = i.view(b, c, h*w)\n",
    "print('Fe shape: ', Fe.shape)\n",
    "print('Fe: ', Fe)\n",
    "print('Fe trans shape: ', Fe.transpose(1,2).shape)\n",
    "G = torch.bmm(Fe, Fe.transpose(1,2))\n",
    "print('G shape: ', G.shape)\n",
    "print('G before div: ', G)\n",
    "G = G.div(c*h*w)\n",
    "print('G after div: ', G)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.ones(1,3,4,4)\n",
    "\n",
    "a, b, c, d = input.size()  # a=batch size(=1)\n",
    "# b=number of feature maps\n",
    "# (c,d)=dimensions of a f. map (N=c*d)\n",
    "\n",
    "features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n",
    "print('features shape: ', features.shape)\n",
    "print('features: ', features)\n",
    "\n",
    "G = torch.mm(features, features.t())  # compute the gram product\n",
    "\n",
    "print('G shape: ', G.shape)\n",
    "print('G: ', G)\n",
    "print('G div: ', G.div(a*b*c*d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "a += [1]\n",
    "print(a)\n",
    "a += [2,3,4,5,6,7]\n",
    "print(a[::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "print(torch.tensor(mean).view(-1, 1, 1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "vgg = models.vgg19(weights='IMAGENET1K_V1').features\n",
    "# vgg.features[0] = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "# print(vgg)\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # normalize img\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "device = 'cuda:0'\n",
    "cnn_normalization_mean = [0.485, 0.456, 0.406]\n",
    "cnn_normalization_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "normalization = Normalization(cnn_normalization_mean, cnn_normalization_std).to(device)\n",
    "model = nn.Sequential(normalization)\n",
    "# class VGG(nn.Module):\n",
    "#     def __init__(self, pool='max'):\n",
    "#         super(VGG, self).__init__()\n",
    "# layers = OrderedDict()\n",
    "# layers['prep'] = normalization\n",
    "i = 0  # increment every time we see a conv\n",
    "for layer in vgg.children():\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        i += 1\n",
    "        name = 'conv_{}'.format(i)\n",
    "    elif isinstance(layer, nn.ReLU):\n",
    "        name = 'relu_{}'.format(i)\n",
    "        # The in-place version doesn't play very nicely with the ContentLoss\n",
    "        # and StyleLoss we insert below. So we replace with out-of-place\n",
    "        # ones here.\n",
    "        layer = nn.ReLU(inplace=False)\n",
    "    elif isinstance(layer, nn.MaxPool2d):\n",
    "        name = 'pool_{}'.format(i)\n",
    "    elif isinstance(layer, nn.BatchNorm2d):\n",
    "        name = 'bn_{}'.format(i)\n",
    "    else:\n",
    "        raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "    \n",
    "    # layers[name] = layer\n",
    "    model.add_module(name, layer)\n",
    "    \n",
    "model.to(device)\n",
    "torch.save(model.state_dict(), 'vgg_conv_0_1.pth')\n",
    "    \n",
    "# for k, v in layers.items():\n",
    "#     print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model_weights = torch.load('vgg_conv_0_1.pth', map_location={'cuda': 'cpu'})\n",
    "\n",
    "from utility.vgg_network import VGG\n",
    "\n",
    "vgg = VGG()\n",
    "vgg.load_state_dict(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (conv1_1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(8, 8))\n",
      "  (fc1): Linear(in_features=32768, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (fc3): Linear(in_features=4096, out_features=26, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from utility.vgg_network_with_top import VGG\n",
    "v = VGG(26)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=26, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "i = torch.randn(8, 1, 256, 256)\n",
    "vgg = models.resnet18(pretrained=False)\n",
    "vgg.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "vgg.fc = nn.Linear(512,26)\n",
    "o = vgg(i)\n",
    "print(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tetta/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SIZE:  41990\n",
      "VAL SIZE:  13624\n",
      "TEST SIZE:  25714\n"
     ]
    }
   ],
   "source": [
    "from train_vgg import make_dataloader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "d = make_dataloader('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(151.0610)\n",
      "tensor(-123.6800)\n"
     ]
    }
   ],
   "source": [
    "print(d['train'].dataset[0][0].max())\n",
    "print(d['train'].dataset[0][0].min())\n",
    "\n",
    "# plt.imshow(d['train'].dataset[0][0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tetta/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/tetta/.local/share/virtualenvs/research-vbjgNxHm/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SIZE:  41990\n",
      "VAL SIZE:  13624\n",
      "TEST SIZE:  25714\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from collections import OrderedDict\n",
    "\n",
    "vgg = models.resnet18(pretrained=False)\n",
    "vgg.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "vgg.fc = nn.Linear(512, 26)\n",
    "\n",
    "\n",
    "model_path = 'resnet18.pth'\n",
    "model_weights = torch.load(model_path, map_location={'cuda': 'cpu'})\n",
    "\n",
    "vgg.load_state_dict(model_weights)\n",
    "\n",
    "from train_vgg import make_dataloader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "d = make_dataloader('cpu')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=26, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "vgg.eval()\n",
    "\n",
    "from utility.utility import load_mono_images\n",
    "\n",
    "f = '../font2img/image/Abel-Regular/D.png'\n",
    "i = load_mono_images(f, 256, 'cpu', 1)\n",
    "i = i.unsqueeze(0)\n",
    "o = vgg(i)\n",
    "# print(i[0][0][100:120, 100:120])\n",
    "print(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "bn1 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "relu ReLU(inplace=True)\n",
      "maxpool MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "layer1 Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "layer2 Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "layer3 Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "layer4 Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "avgpool AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "fc Linear(in_features=512, out_features=26, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "for n, m in vgg.named_children():\n",
    "    print(n, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f671d110130>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from utility.utility import load_images\n",
    "\n",
    "f = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "f_inv = transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225], std=[1/0.229, 1/0.224, 1/0.225])\n",
    "\n",
    "img_path = 'output/265/[1020].jpg'\n",
    "img = load_images(img_path, 256, 'cpu', 1)\n",
    "plt.subplot(131)\n",
    "plt.imshow(img[0].permute(1,2,0))\n",
    "\n",
    "f_img = f(img)\n",
    "plt.subplot(132)\n",
    "plt.imshow(f_img[0].permute(1,2,0))\n",
    "\n",
    "f_invimg = f_inv(f_img)\n",
    "plt.subplot(133)\n",
    "plt.imshow(f_invimg[0].permute(1,2,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.0837, -2.0323, -1.9467,  ..., -2.0837, -2.0837, -2.0837],\n",
       "          [-2.1179, -2.1179, -2.0837,  ..., -2.0837, -2.0837, -2.0837],\n",
       "          [-1.8097, -2.1179, -2.1179,  ..., -2.0837, -2.0837, -2.0837],\n",
       "          ...,\n",
       "          [-2.0494, -2.0494, -2.0494,  ..., -2.0837, -2.0494, -2.0494],\n",
       "          [-2.0494, -2.0494, -2.0494,  ..., -2.0665, -2.0494, -2.0323],\n",
       "          [-2.0494, -2.0494, -2.0494,  ..., -2.0665, -2.0494, -2.0323]],\n",
       "\n",
       "         [[-2.0007, -1.9482, -1.8606,  ..., -2.0007, -2.0007, -2.0007],\n",
       "          [-2.0357, -2.0357, -2.0007,  ..., -2.0007, -2.0007, -2.0007],\n",
       "          [-1.7206, -2.0357, -2.0357,  ..., -2.0007, -2.0007, -2.0007],\n",
       "          ...,\n",
       "          [-1.9657, -1.9657, -1.9657,  ..., -2.0007, -1.9657, -1.9657],\n",
       "          [-1.9657, -1.9657, -1.9657,  ..., -1.9832, -1.9657, -1.9482],\n",
       "          [-1.9657, -1.9657, -1.9657,  ..., -1.9832, -1.9657, -1.9482]],\n",
       "\n",
       "         [[-1.7696, -1.7173, -1.6302,  ..., -1.7696, -1.7696, -1.7696],\n",
       "          [-1.8044, -1.8044, -1.7696,  ..., -1.7696, -1.7696, -1.7696],\n",
       "          [-1.4907, -1.8044, -1.8044,  ..., -1.7696, -1.7696, -1.7696],\n",
       "          ...,\n",
       "          [-1.7347, -1.7347, -1.7347,  ..., -1.7696, -1.7347, -1.7347],\n",
       "          [-1.7347, -1.7347, -1.7347,  ..., -1.7522, -1.7347, -1.7173],\n",
       "          [-1.7347, -1.7347, -1.7347,  ..., -1.7522, -1.7347, -1.7173]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('research-vbjgNxHm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82f3adc9fbda2fc26498424c39d81eed29eee43e13f5d50b10cf9154c73bd2c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
